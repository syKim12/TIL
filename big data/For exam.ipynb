{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0018dc",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define binarizer\n",
    "# threshold 넘으면 1, 아니면 0\n",
    "BI = Binarizer(threshold = 500000, inputCol = 'price', outputCol = 'price_bin')\n",
    "\n",
    "# define string indexer\n",
    "# makes string as a number, for example, belgium is 1 and france is 0\n",
    "SI_lab = StringIndexer(inputCol = 'price_bin', outputCol = 'label')\n",
    "\n",
    "# define one hot encoder for categorical features\n",
    "catColumns = ['waterfront', 'view', 'condition', 'grade', 'renovated']\n",
    "catColumnsIDX = [col + \"_IDX\" for col in catColumns]\n",
    "SI_cat = StringIndexer(inputCols = catColumns, outputCols = catColumnsIDX)\n",
    "\n",
    "# define vector assembler for categorical features\n",
    "# combines a given list of columns into a single vector column\n",
    "VA_cat = VectorAssembler(inputCols = catColumnsIDX, outputCol = 'catFeatures')\n",
    "\n",
    "# define vector assembler for numeric features\n",
    "numColumns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'age', 'bedrooms', 'bathrooms', 'floors']\n",
    "VA_num = VectorAssembler(inputCols = numColumns, outputCol = 'numFeatures')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c43fde",
   "metadata": {},
   "source": [
    "### Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79f169",
   "metadata": {},
   "source": [
    "The classes of all columns can defined manually with the `StructType` and `StructField` command. The latter has three parameters:\n",
    "- `name` of the column\n",
    "- `dataType` of the column\n",
    "- `nullable`which defines whether the column can be null: true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schemas\n",
    "transactionSchema  = StructType([StructField('_c0', IntegerType(), True),\n",
    "                                StructField('InvoiceNo', StringType(), True),\n",
    "                                StructField('StockCode', StringType(), True),\n",
    "  # change the datatype of InvoiceDate from string to timestamp\n",
    "invoices = invoices.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:m\"))                              StructField('Quantity', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9648aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatype of InvoiceDate from string to timestamp\n",
    "invoices = invoices.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83044b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: pyspark DataFrame functions\n",
    "totalSold_2 = transactions.groupBy(\"StockCode\") \\\n",
    "                            .agg({\"Quantity\": \"sum\"}) \\\n",
    "                            .withColumnRenamed(\"sum(Quantity)\", \"totalQuantitySold\") \\\n",
    "                            .filter(\"totalQuantitySold > 25000\") \\\n",
    "                            .sort(\"totalQuantitySold\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of returned deliveries\n",
    "df_ret = transactions.join(inventory, \"StockCode\") \\\n",
    "                    .select(\"InvoiceNo\", (transactions.Quantity * inventory.UnitPrice).alias(\"Revenue\")) \\\n",
    "                    .groupBy(\"InvoiceNo\") \\\n",
    "                    .sum(\"Revenue\") \\\n",
    "                    .withColumnRenamed(\"sum(Revenue)\", \"TotalRevenue\") \\\n",
    "                    .filter(\"TotalRevenue < 0\") \\\n",
    "                    .join(invoices, \"InvoiceNo\") \\\n",
    "                    .groupBy(\"CustomerID\") \\\n",
    "                    .count() \\\n",
    "                    .withColumnRenamed(\"count\", \"nbReturned\")# check\n",
    "df_ret.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "48c1fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerID|nbReturned|\n",
      "+----------+----------+\n",
      "|     13282|         3|\n",
      "|     13610|         2|\n",
      "|     15555|         4|\n",
      "|     15271|         1|\n",
      "|     14157|         1|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "df_ret.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ce150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def add_one(var):\n",
    "    var_new = var + 1\n",
    "    return(var_new)\n",
    "\n",
    "# wrap in udf\n",
    "add_one_udf = udf(add_one, returnType=LongType())\n",
    "\n",
    "# create new column\n",
    "df = df.withColumn(\"x2\", add_one_udf(df.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9a8f205e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---+\n",
      "| id|value|and_one| x2|\n",
      "+---+-----+-------+---+\n",
      "|  A|    5|      6|  6|\n",
      "|  B|   67|     68| 68|\n",
      "|  C|  567|    568|568|\n",
      "+---+-----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1da866",
   "metadata": {},
   "source": [
    "### Regrssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f58572",
   "metadata": {},
   "outputs": [],
   "source": [
    "read data -> create basetable -> pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b49871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the observations containing missing values\n",
    "houses = houses.dropna('any')\n",
    "\n",
    "# Keyword 'any' removes the row if any value of that row is NULL\n",
    "# Keyword 'all' removes the row only if all values of that row are NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001c455",
   "metadata": {},
   "source": [
    "<h4> Pipelines </h4>\n",
    "   \n",
    "    - Because we are working with Big Data processing infrastructure (using distributed processing) the way we code is slightly different than other data processing tools. Here the infrastructure has inherrent built-in functionality that optimizes the way our code is processed. \n",
    "    - In short it means: the program will choose which steps to do when and how they are distributed over the nodes. \n",
    "    - In order for this to be done efficiently we need to give as many instructions as possible at the same time. This way the machine can decide how to divide an conquer. This is done by using pipelines. \n",
    "    - Each step in a pipeline is called a pipeline stage.\n",
    "\n",
    "<br> **Pipelines consist of different stages (transformers & estimators)**, some examples:\n",
    "- **`StringIndexer`**: <br> As a first general step we need to check if there are any text-variabels (usually categorical variables) in the dataset. Not all ML algorithms are able to handle this type of data. That's why it's always good practice to translate textual categories into numerical categories (e.g. A,B,C -> 1,2,3). For our dataset it is not needed.<br> https://spark.apache.org/docs/latest/ml-features.html#stringindexer <br>\n",
    "- **`OneHotEncoderEstimator`**:<br> Another way to handle categorical labels is by transforming them into a vector with 0's and 1's. <br> https://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator <br>\n",
    "- **`VectorIndexer`**: <br> Helps index categorical features in datasets of Vectors. Required for Tree methods.  <br>https://spark.apache.org/docs/latest/ml-features.html#vectorindexer <br>\n",
    "- **`StandardScaler`**: <br> Transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. <br>https://spark.apache.org/docs/latest/ml-features.html#standardscaler <br>\n",
    "- **`VectorAssembler`**: <br> Transforms a number of input columns into one vector. This is used for combining features in order to train ML models like LR and DTs. <br>https://spark.apache.org/docs/latest/ml-features.html#vectorassembler <br>\n",
    "- **Full overview**: <br> https://spark.apache.org/docs/latest/ml-features.html<br> Select only what is needed for the data you have at hand.\n",
    "\n",
    "<br> **Exercice:** Apply several transformers and estimators in the dataset to create the final basetable\n",
    "<br> **NOTE:** Take into account that some transformers need to be applied to the entire dataset, while others need to be applied to train/test set seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables\n",
    "cat_cols = ['waterfront', 'view', 'floors', 'condition', 'grade', 'zipcode', 'renovated', 'bedrooms', 'bathrooms']\n",
    "\n",
    "# define the assembler\n",
    "VA_cat = VectorAssembler(inputCols=cat_cols, outputCol=\"cat_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexer\n",
    "VI = VectorIndexer(inputCol=\"cat_features\", outputCol=\"cat_features_indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a3fac",
   "metadata": {},
   "source": [
    "<h5> Define, fit and apply Pipeline on data </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on data\n",
    "preprocessing_pipeline = Pipeline(stages=[VA_num, VA_cat, VI]).fit(houses)\n",
    "# transform data by applying pipeline model on data\n",
    "preprocessed_data = preprocessing_pipeline.transform(houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca66c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "preprocessed_data = preprocessed_data.select([\"num_features\", \"cat_features_indexed\", \"price\"])\n",
    "# rename price to label\n",
    "preprocessed_data = preprocessed_data.withColumnRenamed(\"price\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be65e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|        num_features|cat_features_indexed|   label|\n",
      "+--------------------+--------------------+--------+\n",
      "|[1180.0,5650.0,11...|[0.0,0.0,0.0,2.0,...|221900.0|\n",
      "|[2570.0,7242.0,21...|[0.0,0.0,2.0,2.0,...|538000.0|\n",
      "|[770.0,10000.0,77...|[0.0,0.0,0.0,2.0,...|180000.0|\n",
      "|[1960.0,5000.0,10...|[0.0,0.0,0.0,4.0,...|604000.0|\n",
      "|[1680.0,8080.0,16...|[0.0,0.0,0.0,2.0,...|510000.0|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "preprocessed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ddc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "The StandardScaler should only be performed on the trainingset, because an equal mean and standard deviation between the training- and testset need to be assumed to avoid methodological mistakes.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cae5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = preprocessed_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# define scaler\n",
    "SC = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")\n",
    "\n",
    "# define assembler\n",
    "VA = VectorAssembler(inputCols=[\"cat_features_indexed\", \"num_features_scaled\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear regression model\n",
    "LR = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# define decision tree model\n",
    "DT = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# define random forest model\n",
    "RF = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720a997",
   "metadata": {},
   "source": [
    "<h5> Define Pipeline for each model and fit on data </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5648e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear regression model pipeline and fit on training data\n",
    "LR_Pipeline = Pipeline(stages=[SC, VA, LR]).fit(train)\n",
    "\n",
    "# define decision tree model pipeline and fit on training data\n",
    "DT_Pipeline = Pipeline(stages=[SC, VA, DT]).fit(train)\n",
    "\n",
    "\n",
    "# define random forest model pipeline and fit on data\n",
    "RF_Pipeline = Pipeline(stages=[SC, VA, RF]).fit(train)\n",
    "<h5> Define Pipeline for each model and fit on data </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50c0d4",
   "metadata": {},
   "source": [
    "<h5> Get predictions on test set by applying each model pipeline on test data </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319879e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of linear regression model on test data\n",
    "lr_preds = LR_Pipeline.transform(test)\n",
    "\n",
    "# get predictions of decision tree model on test data\n",
    "dt_preds = DT_Pipeline.transform(test)\n",
    "\n",
    "# get predictions of random forest model on test data\n",
    "rf_preds = RF_Pipeline.transform(test)\n",
    "<h5> Get predictions on test set by applying each model pipeline on test data </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da5e6b",
   "metadata": {},
   "source": [
    "#### Evaluation -lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9334b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "lrEvaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\") \n",
    "\n",
    "# Get different metrics using your created evaluator object\n",
    "lrsq = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'r2'})\n",
    "lrmae = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'mae'})\n",
    "lrrmse = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'rmse'})\n",
    "lrmse = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'mse'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c09ef3",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80a13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8431037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
