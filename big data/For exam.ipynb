{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecf9b61",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc9d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ef5df4",
   "metadata": {},
   "source": [
    "### Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251a078",
   "metadata": {},
   "source": [
    "The classes of all columns can defined manually with the `StructType` and `StructField` command. The latter has three parameters:\n",
    "- `name` of the column\n",
    "- `dataType` of the column\n",
    "- `nullable`which defines whether the column can be null: true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schemas\n",
    "transactionSchema  = StructType([StructField('_c0', IntegerType(), True),\n",
    "                                StructField('InvoiceNo', StringType(), True),\n",
    "                                StructField('StockCode', StringType(), True),\n",
    "  # change the datatype of InvoiceDate from string to timestamp\n",
    "invoices = invoices.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:m\"))                              StructField('Quantity', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "738d6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatype of InvoiceDate from string to timestamp\n",
    "invoices = invoices.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b074414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: pyspark DataFrame functions\n",
    "totalSold_2 = transactions.groupBy(\"StockCode\") \\\n",
    "                            .agg({\"Quantity\": \"sum\"}) \\\n",
    "                            .withColumnRenamed(\"sum(Quantity)\", \"totalQuantitySold\") \\\n",
    "                            .filter(\"totalQuantitySold > 25000\") \\\n",
    "                            .sort(\"totalQuantitySold\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bf1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of returned deliveries\n",
    "df_ret = transactions.join(inventory, \"StockCode\") \\\n",
    "                    .select(\"InvoiceNo\", (transactions.Quantity * inventory.UnitPrice).alias(\"Revenue\")) \\\n",
    "                    .groupBy(\"InvoiceNo\") \\\n",
    "                    .sum(\"Revenue\") \\\n",
    "                    .withColumnRenamed(\"sum(Revenue)\", \"TotalRevenue\") \\\n",
    "                    .filter(\"TotalRevenue < 0\") \\\n",
    "                    .join(invoices, \"InvoiceNo\") \\\n",
    "                    .groupBy(\"CustomerID\") \\\n",
    "                    .count() \\\n",
    "                    .withColumnRenamed(\"count\", \"nbReturned\")# check\n",
    "df_ret.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b1ca2ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerID|nbReturned|\n",
      "+----------+----------+\n",
      "|     13282|         3|\n",
      "|     13610|         2|\n",
      "|     15555|         4|\n",
      "|     15271|         1|\n",
      "|     14157|         1|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "df_ret.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ea464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def add_one(var):\n",
    "    var_new = var + 1\n",
    "    return(var_new)\n",
    "\n",
    "# wrap in udf\n",
    "add_one_udf = udf(add_one, returnType=LongType())\n",
    "\n",
    "# create new column\n",
    "df = df.withColumn(\"x2\", add_one_udf(df.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4ca58662",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---+\n",
      "| id|value|and_one| x2|\n",
      "+---+-----+-------+---+\n",
      "|  A|    5|      6|  6|\n",
      "|  B|   67|     68| 68|\n",
      "|  C|  567|    568|568|\n",
      "+---+-----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bdc8f2",
   "metadata": {},
   "source": [
    "### Regrssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "read data -> create basetable -> pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the observations containing missing values\n",
    "houses = houses.dropna('any')\n",
    "\n",
    "# Keyword 'any' removes the row if any value of that row is NULL\n",
    "# Keyword 'all' removes the row only if all values of that row are NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adef0f8",
   "metadata": {},
   "source": [
    "<h4> Pipelines </h4>\n",
    "   \n",
    "    - Because we are working with Big Data processing infrastructure (using distributed processing) the way we code is slightly different than other data processing tools. Here the infrastructure has inherrent built-in functionality that optimizes the way our code is processed. \n",
    "    - In short it means: the program will choose which steps to do when and how they are distributed over the nodes. \n",
    "    - In order for this to be done efficiently we need to give as many instructions as possible at the same time. This way the machine can decide how to divide an conquer. This is done by using pipelines. \n",
    "    - Each step in a pipeline is called a pipeline stage.\n",
    "\n",
    "<br> **Pipelines consist of different stages (transformers & estimators)**, some examples:\n",
    "- **`StringIndexer`**: <br> As a first general step we need to check if there are any text-variabels (usually categorical variables) in the dataset. Not all ML algorithms are able to handle this type of data. That's why it's always good practice to translate textual categories into numerical categories (e.g. A,B,C -> 1,2,3). For our dataset it is not needed.<br> https://spark.apache.org/docs/latest/ml-features.html#stringindexer <br>\n",
    "- **`OneHotEncoderEstimator`**:<br> Another way to handle categorical labels is by transforming them into a vector with 0's and 1's. <br> https://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator <br>\n",
    "- **`VectorIndexer`**: <br> Helps index categorical features in datasets of Vectors. Required for Tree methods.  <br>https://spark.apache.org/docs/latest/ml-features.html#vectorindexer <br>\n",
    "- **`StandardScaler`**: <br> Transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. <br>https://spark.apache.org/docs/latest/ml-features.html#standardscaler <br>\n",
    "- **`VectorAssembler`**: <br> Transforms a number of input columns into one vector. This is used for combining features in order to train ML models like LR and DTs. <br>https://spark.apache.org/docs/latest/ml-features.html#vectorassembler <br>\n",
    "- **Full overview**: <br> https://spark.apache.org/docs/latest/ml-features.html<br> Select only what is needed for the data you have at hand.\n",
    "\n",
    "<br> **Exercice:** Apply several transformers and estimators in the dataset to create the final basetable\n",
    "<br> **NOTE:** Take into account that some transformers need to be applied to the entire dataset, while others need to be applied to train/test set seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables\n",
    "cat_cols = ['waterfront', 'view', 'floors', 'condition', 'grade', 'zipcode', 'renovated', 'bedrooms', 'bathrooms']\n",
    "\n",
    "# define the assembler\n",
    "VA_cat = VectorAssembler(inputCols=cat_cols, outputCol=\"cat_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353539f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexer\n",
    "VI = VectorIndexer(inputCol=\"cat_features\", outputCol=\"cat_features_indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2325db",
   "metadata": {},
   "source": [
    "<h5> Define, fit and apply Pipeline on data </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on data\n",
    "preprocessing_pipeline = Pipeline(stages=[VA_num, VA_cat, VI]).fit(houses)\n",
    "# transform data by applying pipeline model on data\n",
    "preprocessed_data = preprocessing_pipeline.transform(houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "preprocessed_data = preprocessed_data.select([\"num_features\", \"cat_features_indexed\", \"price\"])\n",
    "# rename price to label\n",
    "preprocessed_data = preprocessed_data.withColumnRenamed(\"price\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c935eea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|        num_features|cat_features_indexed|   label|\n",
      "+--------------------+--------------------+--------+\n",
      "|[1180.0,5650.0,11...|[0.0,0.0,0.0,2.0,...|221900.0|\n",
      "|[2570.0,7242.0,21...|[0.0,0.0,2.0,2.0,...|538000.0|\n",
      "|[770.0,10000.0,77...|[0.0,0.0,0.0,2.0,...|180000.0|\n",
      "|[1960.0,5000.0,10...|[0.0,0.0,0.0,4.0,...|604000.0|\n",
      "|[1680.0,8080.0,16...|[0.0,0.0,0.0,2.0,...|510000.0|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "preprocessed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "The StandardScaler should only be performed on the trainingset, because an equal mean and standard deviation between the training- and testset need to be assumed to avoid methodological mistakes.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16356b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = preprocessed_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# define scaler\n",
    "SC = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")\n",
    "\n",
    "# define assembler\n",
    "VA = VectorAssembler(inputCols=[\"cat_features_indexed\", \"num_features_scaled\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear regression model\n",
    "LR = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# define decision tree model\n",
    "DT = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# define random forest model\n",
    "RF = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c458d",
   "metadata": {},
   "source": [
    "<h5> Define Pipeline for each model and fit on data </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear regression model pipeline and fit on training data\n",
    "LR_Pipeline = Pipeline(stages=[SC, VA, LR]).fit(train)\n",
    "\n",
    "# define decision tree model pipeline and fit on training data\n",
    "DT_Pipeline = Pipeline(stages=[SC, VA, DT]).fit(train)\n",
    "\n",
    "\n",
    "# define random forest model pipeline and fit on data\n",
    "RF_Pipeline = Pipeline(stages=[SC, VA, RF]).fit(train)\n",
    "<h5> Define Pipeline for each model and fit on data </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c6f58",
   "metadata": {},
   "source": [
    "<h5> Get predictions on test set by applying each model pipeline on test data </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of linear regression model on test data\n",
    "lr_preds = LR_Pipeline.transform(test)\n",
    "\n",
    "# get predictions of decision tree model on test data\n",
    "dt_preds = DT_Pipeline.transform(test)\n",
    "\n",
    "# get predictions of random forest model on test data\n",
    "rf_preds = RF_Pipeline.transform(test)\n",
    "<h5> Get predictions on test set by applying each model pipeline on test data </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e408243",
   "metadata": {},
   "source": [
    "#### Evaluation -lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62293373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "lrEvaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\") \n",
    "\n",
    "# Get different metrics using your created evaluator object\n",
    "lrsq = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'r2'})\n",
    "lrmae = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'mae'})\n",
    "lrrmse = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'rmse'})\n",
    "lrmse = lrEvaluator.evaluate(lr_preds, {lrEvaluator.metricName: 'mse'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c6b55",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a61884",
   "metadata": {},
   "source": [
    "<h5> 1.3.3. Cross Validation</h5>\n",
    "    \n",
    "    - Try to see if you can improve your models performance even more by adding cross-validation into the mix.\n",
    "    - Search the web to understand the concept of Cross Validation.\n",
    "    - Cross validate the random forest model with three values for the `maxDepth`, three values for the `maxBins` and three for the `numTrees`. Use five-fold cross validation.\n",
    "    - Cross validate the the logistic regression with three values for the `regParam`, three values for the `maxIter` and three for the `elasticNetParam`. Use five-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9f5aa",
   "metadata": {},
   "source": [
    "<h5>Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c42ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest model\n",
    "RF = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# define evaluator\n",
    "rf_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# define the parameter space\n",
    "param_grid = (ParamGridBuilder().addGrid(rf_model.maxDepth, [2, 5, 10])\n",
    "                                 .addGrid(rf_model.maxBins, [15, 20, 25])\n",
    "                                 .addGrid(rf_model.numTrees, [5, 20, 50])\n",
    "                                 .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25717a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross validation\n",
    "CV = CrossValidator(estimator=rf_model,\n",
    "                          estimatorParamMaps=param_grid, \n",
    "                          evaluator=rfEvaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training set\n",
    "CV_Pipeline = pipeline(stages=[SC, VA, CV]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get preds on test set\n",
    "cv_preds = CV_Pipeline.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "cv_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a05b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "cv_rmse = cv_evaluator.evaluate(cv_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed75d6",
   "metadata": {},
   "source": [
    "### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias: rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "SS = StandardScaler(inputCol = 'numFeatures', outputCol = 'scaledNumFeatures', withStd = True, withMean = False)\n",
    "\n",
    "# define vector assembler\n",
    "VA = VectorAssembler(inputCols = ['scaledNumFeatures', 'catFeatures'], outputCol = 'features')\n",
    "\n",
    "# define logistic regression model\n",
    "LR = LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [SS, VA, LR]\n",
    "# create pipeline and fit on training set\n",
    "lrModelPipeline = Pipeline().setStages(stages).fit(train)\n",
    "# apply pipeline on test set to get predictions\n",
    "predictions = lrModelPipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# get evaluation metric\n",
    "lrAUC = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "# inspect model performance\n",
    "print('AUC lr: %f' %(lrAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30500c43",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81a9ffa4-ddec-4a2e-9e76-0f1853b87e88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Random Forest**\n",
    "- Build a Random Forest model using the same train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random regression model\n",
    "RF = RandomForestClassifier(labelCol = 'label', featuresCol = 'features')\n",
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# evaluate model\n",
    "rfAUC = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "# inspect model performance\n",
    "print('AUC lr: %f' %(lrAUC))\n",
    "print('AUC rf: %f' %(rfAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b0dce",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc10ea42-7790-43bc-a92e-e08cd4e2a256",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Vector Indexer**\n",
    "\n",
    "As you can see, performance of the Random Forest is lower than the one of Logistic Regression. One reason is the fact that we did not take into account the main advantage of the Random Forest model. This Machine Learning model is able to process real categorical variables. Up until this moment, we fed only binary categoricals to the model by using OneHotEncoding.\n",
    "- Start from the intial houses dataset (at the end of cmd 11).\n",
    "- Transform the data as already done, but replace the OneHotEncoder with a VectorIndexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector indexer is not suitable for linear regression but it works well with decision tree, random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e544d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define binarizer\n",
    "houses = houses.withColumn('price', F.col('price').cast(DoubleType()))\n",
    "BI = Binarizer(threshold = 500000, inputCol = 'price', outputCol = 'price_bin')\n",
    "\n",
    "# define string indexer\n",
    "SI = StringIndexer(inputCol = 'price_bin', outputCol = 'label')\n",
    "\n",
    "# define vector assembler for numeric features\n",
    "numColumns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'age', 'bathrooms', 'floors']\n",
    "VAnum = VectorAssembler(inputCols = numColumns,  outputCol = 'numFeatures')\n",
    "\n",
    "# define vector assembler for categorical features\n",
    "catColumns = ['bedrooms', 'waterfront', 'view', 'condition', 'grade', 'renovated']\n",
    "VAcat = VectorAssembler(inputCols = catColumns, outputCol = 'catFeatures')\n",
    "\n",
    "# define vector indexer\n",
    "VI = VectorIndexer(inputCol = 'catFeatures', outputCol = 'indexedCatFeatures', maxCategories = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b347c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [BI, SI, VAnum, VAcat, VI]\n",
    "# create pipeline and fit on data\n",
    "preprocessingPipeline = Pipeline().setStages(stages).fit(houses)\n",
    "# apply pipeline on data\n",
    "basetable = preprocessingPipeline.transform(houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "SS = StandardScaler(inputCol = 'numFeatures', outputCol = 'scaledNumFeatures', withStd = True, withMean = False)\n",
    "\n",
    "# define vector assembler\n",
    "VA = VectorAssembler(inputCols = ['scaledNumFeatures', 'indexedCatFeatures'], outputCol = 'features')\n",
    "\n",
    "# define random forest model\n",
    "RF = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [SS, VA, RF]\n",
    "# create pipeline and fit on training data\n",
    "rfModelPipeline = Pipeline().setStages(stages).fit(train)\n",
    "# apply pipeline on test data to make predictions\n",
    "predictions = rfModelPipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa291",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "reviews = reviews.withColumn(\"to_lower\", F.lower(F.col(\"verified_reviews\")))\n",
    "\n",
    "# remove numbers\n",
    "reviews = reviews.withColumn(\"no_num\", F.regexp_replace(str=F.col(\"to_lower\"), pattern=\"[0-9]\", replacement=\"\"))\n",
    "\n",
    "# remove punctuation\n",
    "reviews = reviews.withColumn(\"only_str\", F.regexp_replace(str=F.col(\"no_num\"), \n",
    "                                                          pattern=\"[{0}]\".format(re.escape(PUNCTUATION)), \n",
    "                                                          replacement=\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf08725",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline Model 1: Tokenization --> Stop word removal --> BOW --> Logistic Regression\n",
    "Pipeline Model 2: Tokenization --> Stop word removal --> WORD2VEC --> Random Forest\n",
    "Pipeline Model 3: Tokenization --> Stop word removal --> TF-IDF --> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47165a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tokenizer\n",
    "TO = Tokenizer(inputCol=\"only_str\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the stop word remover\n",
    "SWR = StopWordsRemover(inputCol='words', outputCol='filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de065a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the output of the stop word remover\n",
    "temp_pipeline = Pipeline().setStages([TO, SWR]).fit(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bow model\n",
    "BOW = CountVectorizer(inputCol = 'filtered', outputCol = 'features')\n",
    "\n",
    "\n",
    "# define tf model\n",
    "TF = CountVectorizer(inputCol = 'filtered', outputCol = 'featuresTF')\n",
    "# define tf-idf model\n",
    "IdF = IDF(inputCol = 'featuresTF', outputCol = 'features')\n",
    "\n",
    "\n",
    "# define word2vec model\n",
    "W2V = Word2Vec(inputCol = 'filtered', outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the logistic regression model\n",
    "LR = LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter = 100)\n",
    "\n",
    "# define the random forest model\n",
    "RF = RandomForestClassifier(labelCol = 'label', featuresCol = 'features', numTrees = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logistic regression pipeline models and fit on training data\n",
    "lr_BOW_model = Pipeline().setStages([TO, SWR, BOW, LR]).fit(train)\n",
    "lr_TFIDF_model = Pipeline().setStages([TO, SWR, TF, IdF, LR]).fit(train)\n",
    "lr_W2V_model = Pipeline().setStages([TO, SWR, W2V, LR]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest pipeline models and fit on training data\n",
    "rf_BOW_model = Pipeline().setStages([TO, SWR, BOW, RF]).fit(train)\n",
    "rf_TFIDF_model = Pipeline().setStages([TO, SWR, TF, IdF, RF]).fit(train)\n",
    "rf_W2V_model = Pipeline().setStages([TO, SWR, W2V, RF]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of logistic regression pipeline models on validation data\n",
    "lr_BOW_predictions = lr_BOW_model.transform(test)\n",
    "lr_TFIDF_predictions = lr_TFIDF_model.transform(test)\n",
    "lr_W2V_predictions = lr_W2V_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of random forest pipeline models on validation data\n",
    "rf_BOW_predictions = rf_BOW_model.transform(test)\n",
    "rf_TFIDF_predictions = rf_TFIDF_model.transform(test)\n",
    "rf_W2V_predictions = rf_W2V_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a6096",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195eaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", \n",
    "                                          rawPredictionCol=\"probability\", \n",
    "                                          metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f986220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the logistic regression pipeline models in terms of AUC\n",
    "lr_BOW_AUC = evaluator.evaluate(lr_BOW_predictions)\n",
    "lr_TFIDF_AUC = evaluator.evaluate(lr_TFIDF_predictions)\n",
    "lr_W2V_AUC = evaluator.evaluate(lr_W2V_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "# define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                              probabilityCol=\"probability\", \n",
    "                                              metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36230db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the logistic regression pipeline models in temrs of accuracy\n",
    "lr_BOW_ACC = evaluator.evaluate(lr_BOW_predictions)\n",
    "lr_TFIDF_ACC = evaluator.evaluate(lr_TFIDF_predictions)\n",
    "lr_W2V_ACC = evaluator.evaluate(lr_W2V_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55f6ed",
   "metadata": {},
   "source": [
    "inspect total number of 0's and 1's in predictions of random forest pipeline model and real labels\n",
    "-> Always do this check before concluding your model is working properly, even when the AUC and ACC are high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a176e",
   "metadata": {},
   "source": [
    "#### Sentiment Anaylsis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "    \n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5721733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register functions as udf\n",
    "get_sentiment_udf = udf(get_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract positive sentiment score from reviews and store in new columns\n",
    "reviews = reviews.withColumn(\"sentiment\", get_sentiment_udf(\"verified_reviews\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c14845",
   "metadata": {},
   "source": [
    "### Instagram tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2243cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text files into spark dataframe\n",
    "text_df = spark.read.text(all_text_file_paths, wholetext=True) \\\n",
    "                    .withColumnRenamed(\"value\", \"text\") \\\n",
    "                    .withColumn(\"file_path\", F.input_file_name()) \\\n",
    "                    .withColumn(\"post_id\", F.regexp_extract(F.col(\"file_path\"), pattern=\"(raw_data/)(.*)(.txt)\", idx=2)) \\\n",
    "                    .drop(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5256e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define puncutation and stopwords\n",
    "PUNCTUATION = [char for char in punctuation if char not in [\"!\", \"@\", \"#\"]]\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    # remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in PUNCTUATION])\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ad2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to remove stopwords\n",
    "def remove_stops(text_tokenized):\n",
    "    # remove stopwords\n",
    "    text_tokenized = [word for word in text_tokenized if word not in STOPWORDS]\n",
    "    return(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de492dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d71901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register functions as udf\n",
    "remove_punct_udf = F.udf(remove_punct, StringType())\n",
    "remove_stops_udf = F.udf(remove_stops, ArrayType(StringType()))\n",
    "get_hashtags_udf = F.udf(get_hashtags, IntegerType())\n",
    "get_tags_udf = F.udf(get_tags, IntegerType())\n",
    "get_exclamation_marks_udf = F.udf(get_exclamation_marks, IntegerType())\n",
    "get_sentiment_udf = F.udf(get_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from text\n",
    "text_df_f = text_df.withColumn(\"text_lower\", F.lower(\"text\")) \\\n",
    "                 .withColumn(\"text_cleaned\", remove_punct_udf(\"text_lower\")) \\\n",
    "                 .withColumn(\"text_tokenized\", F.split(\"text_cleaned\", \" \")) \\\n",
    "                 .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                 .withColumn(\"num_hashtags\", get_hashtags_udf(\"text_tokenized\")) \\\n",
    "                 .withColumn(\"num_tags\", get_tags_udf(\"text_tokenized\")) \\\n",
    "                 .withColumn(\"num_exclamation_marks\", get_exclamation_marks_udf(\"text_tokenized\")) \\\n",
    "                 .withColumn(\"sentiment\", get_sentiment_udf(\"text\")) \\\n",
    "                 .filter(\"num_words > 0\") \\\n",
    "                 .drop(\"text_tokenized\") \\\n",
    "                 .drop(\"text_lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fd1a0",
   "metadata": {},
   "source": [
    "#### Likes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the binarizer\n",
    "LABEL_BIN = Binarizer(inputCol=\"num_likes\", threshold=29666, outputCol=\"num_likes_bin\")\n",
    "# define indexer\n",
    "LABEL_IDX = StringIndexer(inputCol=\"num_likes_bin\", outputCol=\"label\")\n",
    "# define the pipeline\n",
    "pipeline = Pipeline(stages=[LABEL_BIN, LABEL_IDX]).fit(basetable)\n",
    "# get preprocessed basetable\n",
    "basetable_preprocessed = pipeline.transform(basetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train, val, test = basetable_preprocessed.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea03da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class weights\n",
    "weight_1 = (55 + 161) / (55 * 2)\n",
    "weight_0 = (55 + 161) / (161 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3218fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class weights column\n",
    "train = train.withColumn(\"weight\", F.when(F.col(\"label\") == 1, weight_1).otherwise(weight_0))\n",
    "val = val.withColumn(\"weight\", F.when(F.col(\"label\") == 1, weight_1).otherwise(weight_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categorical variables\n",
    "cat_var = ['ad', 'video', 'location_cat']\n",
    "# define indexed cat var\n",
    "cat_var_idx = [name + \"_idx\" for name in cat_var]\n",
    "# define the indexer\n",
    "CAT_IDX = StringIndexer(inputCols=cat_var, outputCols=cat_var_idx)\n",
    "# define the assembler\n",
    "CAT_VA = VectorAssembler(inputCols=cat_var_idx, outputCol=\"cat_features_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numeric variables\n",
    "num_var = ['num_words',\n",
    "           'num_hashtags',\n",
    "           'num_tags',\n",
    "           'num_exclamation_marks',\n",
    "           'sentiment',\n",
    "           'avg_likes_comments',\n",
    "           'number_ats_comments',\n",
    "           'num_comments',\n",
    "           'num_verified_comments',\n",
    "           'num_followers',\n",
    "           'num_followed']\n",
    "# define the vector assembler\n",
    "NUM_AS = VectorAssembler(inputCols=num_var, outputCol=\"num_features\")\n",
    "# define the scaler\n",
    "NUM_SC = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tokenizer\n",
    "TOK = Tokenizer(inputCol=\"text_cleaned\", outputCol=\"text_tokenized\")\n",
    "# define stop word remover\n",
    "STOP = StopWordsRemover(inputCol=\"text_tokenized\", outputCol=\"text_no_stops\")\n",
    "# define word2vec\n",
    "W2V = Word2Vec(inputCol=\"text_no_stops\", outputCol=\"text_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define final assembler\n",
    "AS = VectorAssembler(inputCols=[\"num_features_scaled\", \"cat_features_idx\", \"text_features\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the models\n",
    "RF_1 = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "RF_2 = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")\n",
    "RF_3 = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\", numTrees=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3fef8",
   "metadata": {},
   "source": [
    "#### Img analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image container to matrix\n",
    "img_matrix = np.array(img_container)\n",
    "# check\n",
    "img_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pca model to reduce dimensionality\n",
    "pca_model = PCA(n_components=10)\n",
    "# fit\n",
    "pca_model = pca_model.fit(img_matrix)\n",
    "# get principal components\n",
    "img_matrix_pca = pca_model.transform(img_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ccc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defgine clustering model\n",
    "kmeans_model = KMeans(n_clusters=10)\n",
    "# fit model\n",
    "kmeans_model = kmeans_model.fit(img_matrix_pca)\n",
    "# get labels\n",
    "img_labels = kmeans_model.predict(img_matrix_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster images\n",
    "clusters_dict = dict()\n",
    "# loop through clusters\n",
    "for i in range(len(img_labels)):\n",
    "    # get label\n",
    "    label = img_labels[i]\n",
    "    # get image\n",
    "    img = img_container[i]\n",
    "    # add to clusters dict\n",
    "    if label not in clusters_dict.keys():\n",
    "        clusters_dict[label] = [img]\n",
    "    else:\n",
    "        clusters_dict[label].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot image\n",
    "def plot_images(clusters_dict, label):\n",
    "    # plot\n",
    "    # get 10 random imgs\n",
    "    random_idx = np.random.choice(range(len(clusters_dict[label])), size=10, replace=False)\n",
    "    random_imgs = [clusters_dict[label][idx] for idx in random_idx]\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    print(\"IMAGES FOR CLUSTER %s\" %label)\n",
    "    for i in range(10): \n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plot_img_array(random_imgs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861b929",
   "metadata": {},
   "source": [
    "### Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define binarizer\n",
    "# threshold 넘으면 1, 아니면 0\n",
    "BI = Binarizer(threshold = 500000, inputCol = 'price', outputCol = 'price_bin')\n",
    "\n",
    "# define string indexer\n",
    "# makes string as a number, for example, belgium is 1 and france is 0\n",
    "SI_lab = StringIndexer(inputCol = 'price_bin', outputCol = 'label')\n",
    "\n",
    "# define one hot encoder for categorical features\n",
    "catColumns = ['waterfront', 'view', 'condition', 'grade', 'renovated']\n",
    "catColumnsIDX = [col + \"_IDX\" for col in catColumns]\n",
    "SI_cat = StringIndexer(inputCols = catColumns, outputCols = catColumnsIDX)\n",
    "\n",
    "# define vector assembler for categorical features\n",
    "# combines a given list of columns into a single vector column\n",
    "VA_cat = VectorAssembler(inputCols = catColumnsIDX, outputCol = 'catFeatures')\n",
    "\n",
    "# define vector assembler for numeric features\n",
    "numColumns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'age', 'bedrooms', 'bathrooms', 'floors']\n",
    "VA_num = VectorAssembler(inputCols = numColumns, outputCol = 'numFeatures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "SS = StandardScaler(inputCol = 'numFeatures', outputCol = 'scaledNumFeatures')\n",
    "\n",
    "# define vector assembler for all features\n",
    "VA_all = VectorAssembler(inputCols = ['scaledNumFeatures', 'catFeatures'], outputCol = 'features')\n",
    "\n",
    "# define logistic regression model\n",
    "GB = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model\n",
    "model_pipeline = Pipeline().setStages([SS, VA_all, GB]).fit(train)\n",
    "# get predictions on test set\n",
    "predictions = model_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94212525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "# get auc\n",
    "lrAUC = evaluator.evaluate(predictions)\n",
    "# print auc\n",
    "print('AUC lr: %f' %(lrAUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ceee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of labels\n",
    "y_true_arr = np.squeeze(np.array(predictions.select(\"label\").collect()))\n",
    "# get array of predicted probabilities\n",
    "y_pred_arr = np.squeeze(np.array(predictions.select(\"probability\").collect()))[:, 1]\n",
    "# get fpr and tpr\n",
    "fpr, tpr, t = roc_curve(y_true_arr, y_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7247c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762bbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
